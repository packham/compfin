{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b> \n",
    "    <font size=\"7\">\n",
    "        Computational Finance and FinTech <br><br>\n",
    "        M.Sc. International Finance\n",
    "    </font>\n",
    "</b>\n",
    "<br><br>\n",
    "<img src=\"pics/HWR.png\" width=400px>\n",
    "<br><br>\n",
    "<b>\n",
    "    <font size=\"5\"> \n",
    "        Prof. Dr. Natalie Packham <br>\n",
    "        Berlin School of Economics and Law <br>\n",
    "        Summer Term 2025\n",
    "    </font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Statistical-Methods-in-Data-Science\" data-toc-modified-id=\"Statistical-Methods-in-Data-Science-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Statistical Methods in Data Science</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge-regression,-Lasso-and-Elastic-Net\" data-toc-modified-id=\"Ridge-regression,-Lasso-and-Elastic-Net-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Ridge regression, Lasso and Elastic Net</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Principal-component-analysis\" data-toc-modified-id=\"Principal-component-analysis-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Principal component analysis</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Methods in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In this part, we study a number of statistical methods that have become very popular in Data Science applications: \n",
    "* Statistical Learning methods are often classified into: \n",
    "  * Regression versus classification\n",
    "  * Supervised versus unsupervised learning (versus reinforcement learning)\n",
    "* The methods studied here cover these different aspects: \n",
    "  * Ridge regression and Lasso (regression, supervised)\n",
    "  * Logistic regression (classification, supervised)\n",
    "  * Principal components analysis (regression, unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression, Lasso and Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In linear regression, we assume a linear relationship between the _target_ $Y$ and the feature vector $X$: \n",
    "\n",
    "  $Y = a + b_1 X_1 + b_2 X_2 + \\cdots + b_m X_m + \\epsilon$,\n",
    "  \n",
    "  where $a, b_1, \\ldots, b_m$ are constants and $\\epsilon$ is the error term.\n",
    "* The ordinary least squares (OLS) estimates of $a,b$ minimise the errors\n",
    "\n",
    "  $\\sum_{i=1}^n \\epsilon^2 = \\sum_{i=1}^n (Y_i - a - b_1 X_{i1} - b_2 X_{i2} - \\cdots - b_m X_{im})^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In machine learning, especially when the number of features is high and when features are highly correlated, overfitting can occur. \n",
    "* One way of dealing with this is known as __regularisation__. \n",
    "* The most popular regularisation methods are: \n",
    "  * Ridge regression\n",
    "  * Lasso\n",
    "  * Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regression\n",
    "\n",
    "* In statistics, __ridge regression__  is known as __Tikhonov regularisation__ or __$L_2$ regularisation__. \n",
    "* Building on OLS, a term is added to the objective function that places a __penalty__ on the size of the coefficients $b_1, \\ldots, b_m$, by minimising:\n",
    "\n",
    "  $\\sum_{i=1}^n (Y_i - a - b_1 X_{i1} - b_2 X_{i2} - \\cdots - b_m X_{im})^2 + \\lambda \\sum_{j=1}^m b_j^2$. \n",
    "* The constant $\\lambda$ is called __tuning parameter__ or __hyperparameter__ and controls the strength of the penalty factor. \n",
    "* The term $\\lambda \\sum_{j=1}^m b_j^2$ is called the __shrinkage penalty__, as it will shrink the estimates of $b_1, \\ldots, b_m$ towards zero. \n",
    "* Selecting a good value of $\\lambda$ is critial and can be achieved, for example, by __cross-validation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regression \n",
    "\n",
    "* The OLS estimates do not depend on the magnitude of the independent variables: multiplying $X_j$ by a constant $c$ leads to a scaling of the OLS-coefficient by $1/c$. \n",
    "* This is different in ridge regression (and Lasso, see below): the estimated coefficients can change substantially when re-scaling independent variables. \n",
    "* Therefore, it is custom, to _standardise_ the features: \n",
    "\n",
    "  $\\tilde x_{ij} = \\displaystyle \\frac{x_{ij}} {\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij}-\\overline{x}_j)^2}}$, \n",
    "\n",
    "  so that all variables are on the same scale, i.e., they all have a standard deviation of one.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso\n",
    "\n",
    "* __Lasso (Least absolute shrinkage and selection operator)__, also known as __$L_1$ regularisation__ adds a different penalty: \n",
    "\n",
    "  $\\sum_{i=1}^n (Y_i - a - b_1 X_{i1} - b_2 X_{i2} - \\cdots - b_m X_{im})^2 + \\lambda \\sum_{j=1}^m |b_j|$. \n",
    "* This has the interesting effect that the less relevant features are completely eliminated. \n",
    "* For this reason, Lasso is also often used as a feature selection or variable selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Elastic net regression\n",
    "\n",
    "* __Elastic net regression__ is a mixture of ridge regression and Lasso: \n",
    "\n",
    "  $\\sum_{i=1}^n (Y_i - a - b_1 X_{i1} - b_2 X_{i2} - \\cdots - b_m X_{im})^2 + \\lambda_1 \\sum_{j=1}^m b_j^2 +  \\lambda_2 \\sum_{j=1}^m |b_j|$. \n",
    "\n",
    "* Combining the effects of ridge regression and Lasso means that simultaneously\n",
    "  * some coefficients are reduced to zero (Lasso), \n",
    "  * some coefficients are reduced in size (ridge regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* The following application predicts house prices based on different features of the property. \n",
    "* The data set is from \n",
    "\n",
    "  Hull: Machine Learning in Business. 3rd edition, independently published, 2021. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # python's data handling package\n",
    "import numpy as np # python's scientific computing package\n",
    "import matplotlib.pyplot as plt # python's plotting package\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "# The sklearn library has cross-validation built in!\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both features and target have already been scaled: mean = 0; SD = 1\n",
    "data = pd.read_csv('data/Houseprice_data_scaled.csv') \n",
    "# data = pd.read_csv('https://raw.githubusercontent.com/packham/Python_CFDS/main/data/Houseprice_data_scaled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Sale Price', axis=1)\n",
    "y = data['Sale Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* `sklearn` can split training and testing data randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)\n",
    "mse(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Observe how the OLS coefficients are all non-zero. \n",
    "* However, some coefficients are negative where a positive coefficient would be expected (e.g. `FullBath`). \n",
    "* This is an indication that the model is struggling to fit the large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataFrame with corresponding feature and its respective coefficients\n",
    "coeffs = pd.DataFrame([['intercept'] + list(X_train.columns),[lr.intercept_] + lr.coef_.tolist()]).transpose().set_index(0)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Indeed, some correlations are high, as the heatmap indicates, which may cause multicollinearity and ill-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Ridge\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train on the training data set and use  test data set to determine test MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=np.int(len(X)*.75) # choose 75% of data for training\n",
    "# The alpha used by Python's ridge should be the lambda above times the number of observations\n",
    "alphas=[0.01*n, 0.02*n, 0.03*n, 0.04*n, 0.05*n, 0.075*n,0.1*n,0.125*n, 0.15*n,0.2*n, 0.4*n]\n",
    "mses=[]\n",
    "for alpha in alphas:\n",
    "    ridge=Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    pred=ridge.predict(X_test)\n",
    "    mses.append(mse(y_test,pred))\n",
    "mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* This is how to use cross-validation; just specify the number of folds (`cv`) and MSE as the `scoring` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alpha used by Python's ridge should be the lambda above times the number of observations\n",
    "alphas=[0.01*n, 0.02*n, 0.03*n, 0.04*n, 0.05*n, 0.075*n,0.1*n,0.125*n, 0.15*n,0.2*n, 0.4*n]\n",
    "mses=[]\n",
    "for alpha in alphas:\n",
    "    scores = cross_val_score(Ridge(alpha=alpha), X, y, cv=4, scoring=\"neg_root_mean_squared_error\")**2\n",
    "    mses.append(np.mean(scores))\n",
    "#np.transpose([alphas, mses])\n",
    "mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Average test MSE varies with the hyperparameter $\\alpha$. \n",
    "* The best model choice is at approximately $0.1\\cdot n = 218$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we produce results for alpha=0.05 which corresponds to lambda=0.1 in Hull's book\n",
    "lasso = Lasso(alpha=0.05)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* As Lasso acts as a variable selection method we would expect some coefficients to be set to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with corresponding feature and its respective coefficients\n",
    "coeffs = pd.DataFrame(\n",
    "    [\n",
    "        ['intercept'] + list(X_train.columns),\n",
    "        [lasso.intercept_] + lasso.coef_.tolist()\n",
    "    ]\n",
    ").transpose().set_index(0)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Now, let's find again the parameter with minimal average test MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We now consider different lambda values. The alphas are half the lambdas\n",
    "alphas=[0.0025/2, 0.005/2, 0.01/2, 0.015/2, 0.02/2, 0.025/2, 0.03/2, 0.04/2, 0.05/2]\n",
    "mses=[]\n",
    "for alpha in alphas:\n",
    "    scores = cross_val_score(Lasso(alpha=alpha), X, y, cv=4, scoring=\"neg_root_mean_squared_error\")**2\n",
    "    mses.append(np.mean(scores))    \n",
    "#np.transpose([alphas,mses])\n",
    "mses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The optimal parameter is at $\\alpha=0.0075$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In a regression setting, numerical variables are predicted. \n",
    "* Another application is classification, which is about predicting the category a new observation belongs to. \n",
    "* In supervised learning, and with two categories, a variation of regression, called __logistic regression__ can be used. \n",
    "* Given features $X_1, \\ldots, X_m$, suppose there are two classes to which observations can belong. \n",
    "* An example is the prediction of a loan's default risk, given characteristics of the creditor such as age, education, marital status, etc. \n",
    "* Another example is the classification of e-mails into junk or non-junk e-mails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "* Logistic regression can be used to calculate the probability of a positive outcome via the __sigmoid function__ \n",
    "\n",
    "  $P(Y=1|X) = \\displaystyle \\frac{1}{1+\\mathrm{e}^{-X}} = \\frac{\\mathrm{e}^{X}}{1+\\mathrm{e}^X}$,\n",
    "  \n",
    "  where $\\mathrm{e}$ is the Euler constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(-7,7,0.25)\n",
    "plt.plot(x, 1/(1+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "* Setting $X=a + b_1 X_1 + b_2 X_2 + \\cdots + b_m X_m$, the probability of a positive outcome is \n",
    "\n",
    "  $P(Y=1|X_1,\\ldots, X_m) = \\displaystyle \\frac{1}{1+\\textrm{exp}(-a - \\sum_{j=1}^m b_j X_j)}$. \n",
    "  \n",
    "* The objective is to find the coefficients $a, b_1, \\ldots, b_m$ that best classify the given data. \n",
    "* __Maximum likelihood__ is a versatile method for this type of problem, when OLS does not apply. \n",
    "* Without going into detail, the __log likelihood function__ is given as \n",
    "\n",
    "  $\\ell(a, b_1, \\ldots, b_m|x_1, \\ldots, x_n) = \\sum_{k:y_k=1} \\ln (p(x_k)) + \\sum_{k: y_k=0} \\ln (1-p(x_k))$,\n",
    "  \n",
    "  and the parameters are chosen that maximise this function. \n",
    "\n",
    "* (Note: The likelihood function is derived by considering the observations to be independent outcomes of a Bernoulli random variable.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Credit risk\n",
    "\n",
    "* The dataset in this example is taken from James et al.: An Introduction to Statistical Learning. Springer, 2013.\n",
    "\n",
    "* It contains simulated data of defaults on credit card payments, on the basis of credit card balance (amongst other things).\n",
    "\n",
    "* An excellent tutorial and examples on logistic regression in Python is available here: https://realpython.com/logistic-regression-python/.\n",
    "\n",
    "* We will use the `sklearn` package below. Logistic regression can also be performed with the `statsmodels.api`, in which case $p$-values and other statistics are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Default_JamesEtAl.csv\")\n",
    "# data = pd.read_csv(\"https://raw.githubusercontent.com/packham/Python_CFDS/main/data/Default_JamesEtAl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(data[\"balance\"]).reshape(-1,1) # array must be two-dimensional\n",
    "y=np.array([True if x==\"Yes\" else False for x in data[\"default\"]]) # list comprehension\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted parameters\n",
    "a=model.intercept_[0]\n",
    "b=model.coef_[0,0]\n",
    "[a,b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Scatter plot of data and fitted logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y,c='orange', marker=\"+\")\n",
    "plt.xlabel('balance')\n",
    "xrange=range(0,3000,10)\n",
    "plt.plot(xrange,1/(1+np.exp(-a-b *xrange)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(x_train)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_train)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.score(x_train,y_train), model.score(x_test,y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Confusion matrix: \n",
    "\n",
    "<center>\n",
    "<img src=\"pics/confusionMatrix.png\" width=300px>\n",
    "    </center>\n",
    "    <div align=\"right\" style=\"font-size:14px\">https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train,model.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,model.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* See link below for an explanation of the metrics:\n",
    "\n",
    "https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, model.predict(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Principal Component Analysis (PCA)__ summarises a large set of correlated variables by smaller number of representative variables that explain most of the variability of the original data set.\n",
    "* It is a standard method for reducing the dimension of high dimensional, highly correlated systems.\n",
    "* The principal components are common factors that are unobservable (latent) and directly estimated from the data. \n",
    "* While being explanatory in a statistical sense, the factors do not necessarily have an economic interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "<center>\n",
    "    <table><tr>\n",
    "        <td><img src=\"pics/pcaexample.png\" width=500px></td>\n",
    "        <td><img src=\"pics/pca3.png\" width=500px></td> \n",
    "        </tr>\n",
    "    </table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "* The objective is to take $p$ random variables $X_1, X_2, \\ldots, X_p$ and find (linear) combinations of these to produce random variables $Z_1, \\ldots, Z_p$, the __principal components__, that are uncorrelated. \n",
    "* Geometrically, expressing $X_1,\\ldots, X_p$ through linear combinations $Z_1,\\ldots, Z_p$ can be thought of as shifting and rotating the axes of the coordinate system (see left graph). \n",
    "* Hence, the transform leaves the data points unchanged, but expresses them using different coordinates. \n",
    "* The lack of correlation is a useful property because it means that the principal components are measuring different \"dimensions\" of the data. \n",
    "* The principal components can be ordered according to their variance, that is, $\\text{Var}(Z_1)\\geq \\text{Var}(Z_2)\\geq \\cdots \\geq \\text{Var}(Z_p)$. \n",
    "* If the variance captured in the higher dimensions is sufficiently small, then discarding those higher dimensions will retain most of the variability, so only little information is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "* Let $(X, Y)$ be are standard normally distributed random variables with a correlation of $0.7$. \n",
    "* The left graph above shows a scatterplot of a sample of 100 random numbers $(x_1, y_1), \\ldots, (x_{100},  y_{100})$. \n",
    "* By shifting and rotating the axes, new variables $Z_1, Z_2$, the principal components, with $Z_i=a_i X + b_i Y$ are obtained. \n",
    "* The data sample expressed in terms of $Z_1, Z_2$ is uncorrelated.\n",
    "* Also, the variance of the data contribution from the first principal component $Z_1$ is much greater than the variance contribution from the second principal component.\n",
    "* The graph on the right shows the data points when the data are onto the first principal component, discarding the second dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "* The sample variance of $(x_1, \\ldots, x_n)$ is $0.9244$ and the sample variance of $(y_1, \\ldots, y_n)$ is $0.9226$, whereas the sample variance of the data in the first principal component is $1.6014$ and of the second principal component is $0.2456$. \n",
    "* In other words, while the original axes each account for roughly $50\\%$ of the total variance, the first principal component accounts for $87\\%$ of the sample variance.\n",
    "* Neglecting the second principal component and expressing the data in the first principal component only retains $87\\%$ of the variance (see right graph. \n",
    "* In practice, the number of dimensions will be higher, and one will choose the number of principal components to reflect a certain variance contribution such as $99\\%$.\n",
    "* If the data are sufficiently correlated, then only few dimensions (principal components) will be required even for high-dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA example for interest rate term structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interest rates of different maturities are known to exhibit large correlations. \n",
    "* Interest rate term structures are therefore a good candidate for a representation by a few factors only. \n",
    "* What do you think are the main ways in which an interest term structure moves over time?\n",
    "* The data below is taken from \n",
    "\n",
    "  Hull: Machine Learning in Business. 3rd edition, independently published, 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(\"./data/Treasuries_Hull.xlsx\", index_col=0, parse_dates=True)\n",
    "#data=pd.read_excelI\"https://raw.githubusercontent.com/packham/Python_CFDS/main/data/Treasuries_Hull.xlsx\", index_col=0, parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Our interest lies in movements of interest rate term structures, therefore we take first differences of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=100*data.diff();\n",
    "d.dropna(inplace=True)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Correlations of term structure movements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(d.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The principal components are the eigenvectors of the correlation matrix.\n",
    "* These are also called factor loadings. They express the \"weight\" of each factor for each maturity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, vr=linalg.eig(d.corr()) # eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The first PC captures changes in the level the term structure. \n",
    "* The second PC captures changes in the slope. \n",
    "* The third PC can be interpreted as a hump in the term structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.xticks(range(len(d.columns)), d.columns, rotation='vertical')\n",
    "plt.plot(np.real(vr[:,0:3])); # first factor loadings\n",
    "plt.legend([\"PC 1\", \"PC 2\", \"PC 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The eigenvalues express the variance captured by each PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.real(100*w/w.sum()) # percentage variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The principal component scores are the original data expressed in the PC coordinate system, dimension by dimension. \n",
    "* These are obtained by multiplying each PC vector with the original data (de-meaned). \n",
    "* Since the principal components are determined from maximising the variance explained by the model, this can be used to interpret the principal components. \n",
    "* The correlation of each score with the original data allow for an interpretation of what each principal component represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = np.transpose(vr[:,0]*(d-d.mean())).sum()\n",
    "pc2 = np.transpose(vr[:,1]*(d-d.mean())).sum()\n",
    "pc3 = np.transpose(vr[:,2]*(d-d.mean())).sum()\n",
    "pc4 = np.transpose(vr[:,3]*(d-d.mean())).sum()\n",
    "pc5 = np.transpose(vr[:,4]*(d-d.mean())).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This gives the same result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.transpose(np.matmul(np.transpose(vr), np.transpose(d.values-d.values.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    d.insert(i, 'pc' + str(i), pc[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The plot below shows the correlations of each interest rate with the first five PC's. \n",
    "* Note how the interpretation is similar to the factor loadings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(np.transpose(d.corr().head(5)).iloc[5:])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_metadata": {
   "author": "Prof.\\ Dr.\\ Natalie Packham\\\\ DVFA \\\\ Chartered Financial Data Scientist\\\\ Summer Term 2020",
   "bib": "notebook.bib",
   "title": "Financial Time Series"
  },
  "livereveal": {
   "slideNumber": "c"
  },
  "toc": {
   "base_numbering": "9",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.997px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
