{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b> \n",
    "    <font size=\"7\">\n",
    "        Computational Finance and FinTech <br><br>\n",
    "        M.Sc. International Finance\n",
    "    </font>\n",
    "</b>\n",
    "<br><br>\n",
    "<img src=\"pics/HWR.png\" width=400px>\n",
    "<br><br>\n",
    "<b>\n",
    "    <font size=\"5\"> \n",
    "        Prof. Dr. Natalie Packham <br>\n",
    "        Berlin School of Economics and Law <br>\n",
    "        Summer Term 2025\n",
    "    </font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Financial-Time-Series\" data-toc-modified-id=\"Financial-Time-Series-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Financial Time Series</a></span><ul class=\"toc-item\"><li><span><a href=\"#Financial-Data\" data-toc-modified-id=\"Financial-Data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Financial Data</a></span></li><li><span><a href=\"#Correlation-analysis-and-linear-regression\" data-toc-modified-id=\"Correlation-analysis-and-linear-regression-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Correlation analysis and linear regression</a></span></li><li><span><a href=\"#Time-series-models:-Empirical-stylised-facts\" data-toc-modified-id=\"Time-series-models:-Empirical-stylised-facts-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Time series models: Empirical stylised facts</a></span></li><li><span><a href=\"#Time-Series-Models:-GARCH\" data-toc-modified-id=\"Time-Series-Models:-GARCH-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Time Series Models: GARCH</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Financial Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Further reading: __Py4Fi, Chapter 8__\n",
    "* This session also covers material not in __Py4Fi__.\n",
    "* Time series are ubiquitous in finance. \n",
    "* `pandas` is the main library in Python to deal with time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Financial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Financial data\n",
    "\n",
    "* For the time being we work with locally stored data files.\n",
    "* These are in `.csv`-files (comma-separated values), where the data entries in each row are separated by commas. \n",
    "* Some initialisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import\n",
    "* `pandas` provides a numer of different functions and `DataFrame` methods for importing and exporting data.\n",
    "* Here we use `pd.read_csv()`.\n",
    "* The file that we load contains end-of-day data for different financial instruments retrieved from Thomson Reuters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "filename = './data/tr_eikon_eod_data.csv' # path and filename\n",
    "f = open(filename, 'r')  \n",
    "f.readlines()[:5]  # show first five lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename,  # import csv-data into DataFrame\n",
    "                   index_col=0, # take first column as index\n",
    "                   parse_dates=True)  # index values are datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.info()  # information about the DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.tail()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.plot(figsize=(10, 10), subplots=True);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import\n",
    "\n",
    "* The identifiers used by Thomson Reuters are so-called RIC's. \n",
    "* The financial instruments in the data set are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "instruments = ['Apple Stock', 'Microsoft Stock',\n",
    "               'Intel Stock', 'Amazon Stock', 'Goldman Sachs Stock',\n",
    "               'SPDR S&P 500 ETF Trust', 'S&P 500 Index',\n",
    "               'VIX Volatility Index', 'EUR/USD Exchange Rate',\n",
    "               'Gold Price', 'VanEck Vectors Gold Miners ETF',\n",
    "               'SPDR Gold Trust']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for ric, name in zip(data.columns, instruments):\n",
    "    print('{:8s} | {}'.format(ric, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.describe().round(2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary statistics\n",
    "* The `aggregate()`-function allows to customise the statistics viewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.aggregate([min,  \n",
    "                np.mean,  \n",
    "                np.std,  \n",
    "                np.median,  \n",
    "                max]  \n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns\n",
    "* When working with financial data we typically (=always - you must have good reasons to deviate from this) work with performance data, i.e., __returns__. \n",
    "* Reasoning: \n",
    "     * Historical data are mainly used to make forecasts one or several time periods forward. \n",
    "     * The daily average stock price over the last eight years is meaningless to make a forecast for tomorrow's stock price. \n",
    "     * However, the daily returns are possible scenarios for the next time period(s). \n",
    "* The function `pct_change()` calculates discrete returns: \n",
    "$$r_t^{\\rm d}=\\frac{S_{t}-S_{t-1}}{S_{t-1}},$$\n",
    "     where $S_t$ denotes the stock price at time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.pct_change().round(3).head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.pct_change().mean().plot(kind='bar', figsize=(10, 6));  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns\n",
    "* In finance, __log-returns__, also called __continuous returns__, are often preferred over discrete returns: \n",
    "$r_t^{\\rm c} = \\ln\\left(\\frac{S_t}{S_{t-1}}\\right).$\n",
    "* The main reason is that log-return are additive over time. \n",
    "* For example, the log-return from $t-1$ to $t+1$ is the sum of the single-period log-returns: \n",
    "$$r_{t-1,t+1}^{\\rm c} = \\ln \\left(\\frac{S_{t+1}}{S_t}\\right) + \\ln \\left(\\frac{S_t}{S_{t-1}}\\right) = \\ln\\left(\\frac{S_{t+1}}{S_t}\\cdot \\frac{S_t}{S_{t-1}}\\right) = \\ln\\left(\\frac{S_{t+1}}{S_{t-1}}\\right).$$\n",
    "* Note: If the sampling (time) interval is small (e.g. one day or one week), then the difference between discrete returns and log-returns is negligible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets = np.log(data / data.shift(1))  # calculates log-returns in a vectorised way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets.head().round(3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets.cumsum().apply(np.exp).plot(figsize=(10, 6));  # recover price paths from log-returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resampling\n",
    "* Down-sampling is achieved by `resample()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.resample('1w', label='right').last().head()  # down-sample to weekly time intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation analysis and linear regression\n",
    "* To further illustrate how to work with financial time series we consider the S&P 500 stock index and the VIX volatility index. \n",
    "* Empirical stylised fact: As the S&P 500 rises, the VIX falls, and vice versa. \n",
    "* Note: This is about __correlation__ not __causation__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# EOD data from Thomson Reuters Eikon Data API\n",
    "raw = pd.read_csv('./data/tr_eikon_eod_data.csv', index_col=0, parse_dates=True)\n",
    "data = raw[['.SPX', '.VIX']].dropna()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data.plot(subplots=True, figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis\n",
    "* Transform both data series into log-returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets = np.log(data / data.shift(1)) \n",
    "rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets.dropna(inplace=True) # drop NaN (not-a-number) entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets.plot(subplots=True, figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(rets,  \n",
    "                           alpha=0.2,  \n",
    "                           diagonal='hist',  \n",
    "                           hist_kwds={'bins': 35},  \n",
    "                           figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rets.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression\n",
    "* __Linear regression__ captures the linear relationship between two variables. \n",
    "* For two variables $x,y$, we postulate a linear relationship: \n",
    "$$ y = \\alpha + \\beta x + \\varepsilon, \\quad \\alpha, \\beta\\in \\mathbb{R}.$$\n",
    "* Here, $\\alpha$ is the __intercept__, $\\beta$ is the __slope (coefficient)__ and $\\varepsilon$ is the __error term__. \n",
    "* Given  data sample of joint observations $(x_1,y_1), \\ldots, (x_n,y_n)$, we set \n",
    "$$ y_i = \\hat\\alpha + \\hat\\beta x_i + \\hat\\varepsilon_i,$$\n",
    "where $\\hat\\alpha$ and $\\hat\\beta$ are estimates of $\\alpha,\\beta$ and $\\hat\\varepsilon_1,\n",
    "\\ldots, \\hat\\varepsilon_n$ are the so-called __residuals__. \n",
    "* The __ordinary least squares (OLS)__ estimator $\\hat\\alpha,\\hat\\beta$ corresponds to those values of $\\alpha,\\beta$ that minimise the sum of squared residuals: \n",
    "$$\\min_{\\alpha,\\beta} \\sum_{i=1}^n \\varepsilon_i^2 = \\sum_{i=1}^n (y_i-\\alpha-\\beta x_i)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regressions\n",
    "* Simplest form of OLS regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "reg = np.polyfit(rets['.SPX'], rets['.VIX'], deg=1)  # fit a linear equation (a polynomial of degree 1)\n",
    "reg.view() # the fitted paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ax = rets.plot(kind='scatter', x='.SPX', y='.VIX', figsize=(8, 5)) \n",
    "ax.plot(rets['.SPX'], np.polyval(reg, rets['.SPX']), 'r', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression\n",
    "* To do a more refined OLS regression with a proper analysis, use the package `statsmodels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "Y=rets['.VIX']\n",
    "X=rets['.SPX']\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "results.predict()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression: Interpretation of output and forecasting\n",
    "* The column `coef` lists the coefficients of the regression: the coefficient in the row labelled `const` corresponds to $\\hat\\alpha$ ($=0.0026$) and the coefficient in the row `.SPX` denotes $\\hat\\beta$ ($=-6.6515$). \n",
    "* The estimated model in the example is thus: \n",
    "$$\n",
    "\\text{.VIX} = 0.0026 - 6.6516 \\text{.SPX}. \n",
    "$$\n",
    "* The best forecast of the VIX return when observing an S&P return of 2% is therefore $0.0026 - 6.6516\\cdot 0.02 = -0.130432 = -13.0432\\%$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression: Validation ($R^2$)\n",
    "* To __validate__ the model, i.e., to determine, if the model in itself and the explanatory variable(s) make sense, we look $R^2$ and various $p$-values (or confidence intervals or $t$-statistics). \n",
    "* $R^2$ measures the fraction of variance in the dependent variable $Y$ that is captured by the regression line; $1-R^2$ is the fraction of $Y$-variance that remaines in the residuals $\\varepsilon_i^2$, $i=1,\\ldots, n$. \n",
    "* In the output above $R^2$ is given as $0.647$. In other words, $64.7\\%$ of the variance in VIX returns are \"explained\" by SPX returns. \n",
    "* A high $R^2$ (and this one is high) is necessary for making forecasts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression: Validation (confidence interval)\n",
    "* An important hypothesis to test in any regression model is whether the explanatory variable(s) have an effect on the independent variable. \n",
    "* This can be translated into testing whether $\\beta\\not=0$. ($\\beta=0$ is the same as saying that the $X$ variable can be removed from the model.)\n",
    "* Formally, we test the null hypothesis $H_0: \\beta=0$ against the alternative hypothesis $H_1: \\beta\\not=0$. \n",
    "* There are several statistics to come to the same conclusion: confidence intervals, $t$-statistics and $p$-values. \n",
    "* The __confidence interval__ is an interval around the estimate $\\hat\\beta$ that we are confident contains the true parameter $\\beta$. A typial __confidence level__ is 95%. \n",
    "* If the 95% confidence interval does __not__ contain 0, then we say $\\beta$ is __statistically significant__ at the 5% (=1-95%) level, and we conclude that $\\beta\\not=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression: Validation ($t$-statistic)\n",
    "* The $t$-statistic corresponds to the __number of standard deviations__ that the estimated coefficient $\\hat\\beta$ is away from $0$ (the mean under $H_0$). \n",
    "* For a normal distribution, we have the following rules of thumb: \n",
    "    * $66\\%$ of observations lie within one standard deviation of the mean\n",
    "    * $95\\%$ of observations lie within two standard deviations of the mean\n",
    "    * $99.7\\%$ of observations lie within three standard deviations of the mean  \n",
    "<center>\n",
    "<img src=\"pics/normal6.png\" width=400px>\n",
    "</center>\n",
    "* If the sample size is large enough, then the $t$-statistic is approximately normally distributed, and if it is large (in absolute terms), then this is an indication against $\\beta=0$. \n",
    "* In the example above, the $t$-statistics is -62.559, i.e., $\\hat\\beta$ is approx. 63 standard deviations away from zero, which is practically impossible. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### OLS regression: Validation ($p$-value)\n",
    "* The $p$-value expresses the probability of observing a coefficient estimate as extreme (away from zero) as $\\hat\\beta$ under $H_0$, i.e., when $\\beta=0$. \n",
    "* In other words, it measures the probability of observing a $t$-statistic as extreme as the one observed if $\\beta=0$. \n",
    "* If the $p$-value (column ``P>|t|``) is smaller than the desired level of significance (typically 5%), then the $H_0$ can be rejected and we conclude that $\\beta\\not=0$. \n",
    "* In the example above, the $p$-value is given as $0.000$, i.e., it is so small, that we can conclude the estimated coefficient $\\hat\\beta$ is so extreme (= away from zero) that is virtually impossible to obtain such an estimated if $\\beta=0$. \n",
    "\n",
    "* Finally, the $F$-test tests the hypotheses $H_0:R^2=0$ versus $H_1:R^2\\not=0$. In a multiple regression with $k$ independent variables, this is equivalent to $H_0: \\beta_1=\\cdots=\\beta_k=0$. \n",
    "* In the example above, the $p$-value of the $F$-test is $0$, so we conclude that the model overall has explanatory power. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time series models: Empirical stylised facts\n",
    "* We discuss empirical stylised facts of financial time series. \n",
    "* The GARCH model is the standard workhorse in financial time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* Load data set containing of daily DAX closing prices (1990-2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dax = pd.read_csv('./data/yahoo_GDAXI.csv',index_col = 0,na_values = 'null')\n",
    "dax.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* Transform closing prices to log returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data=dax['Close']\n",
    "returns = 100*np.log(data / data.shift(1))\n",
    "returns.dropna(inplace=True)\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "returns.plot(figsize=(15,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "*  When working with a data sample, we often assumes that the data are independent and identically distributed (\"iid\"). \n",
    "* The previous plot shows that the \"iid\" assumption is violated.\n",
    "* The \"iid\" assumption is in general not justified for financial data, and more sophisticated models for time series are more appropriate for capturing phenomena such as volatility clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* An __empirical stylised fact__ of a financial time series is an empirical observations that applies to the majority of (daily) series of asset returns, such as log-returns of equities, indexes, FX rates and commodity prices (see Mcneil, 2005, and Cont, 2001).\n",
    "* Generally accepted stylised facts of asset returns are: \n",
    "     1. Return series are not iid although they show little serial correlation.\n",
    "     2. Series of absolute or squared returns show profound serial correlation.\n",
    "     3. Conditional expected returns are close to zero.\n",
    "     4. Volatility appears to vary over time.\n",
    "     5. Return series are leptokurtic or heavy-tailed.\n",
    "     6. Extreme returns appear in clusters.\n",
    "<font size=\"2\">  \n",
    "  A.J. McNeil, R. Frey, and P. Embrechts. Quantitative Risk\n",
    "  Management. Princeton University Press, Princeton, NJ, 2005.\n",
    "    <br>\n",
    "  R. Cont. Empirical properties of asset returns: stylized\n",
    "  facts and statistical issues. Quantitative Finance,\n",
    "  1(2):223–236, 2001. \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* The figure below illustrates the first three stylised facts (serial correlation = autocorrelation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ac = [];\n",
    "acabs=[];\n",
    "for i in range(0,30):\n",
    "    ac.append(returns.autocorr(i))\n",
    "    acabs.append(abs(returns).autocorr(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* The figure below illustrates the first three stylised facts (serial correlation = autocorrelation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,3))\n",
    "fig.suptitle(\"DAX autocorrelation. Left: returns; right: absolute returns\")\n",
    "plt.subplot(121)\n",
    "plt.bar(range(0,30), ac);\n",
    "plt.xlabel('days');\n",
    "plt.ylabel('autocorrelation');\n",
    "plt.subplot(122)\n",
    "plt.bar(range(0,30), acabs);\n",
    "plt.xlabel('days');\n",
    "plt.ylabel('autocorrelation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* The excess kurtosis of the DAX returns suggests that more extreme events occurs than a normal distribution would suggest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "returns.kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* The following figures shows the DAX volatility based on a rolling time windows of 252 trading days (approx. one year). \n",
    "* This illustrates that volatility varies over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "vol=returns.rolling(window=252).std()\n",
    "vol.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "vol.plot(figsize=(10,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* The following figure illustrates the 100 most extreme DAX returns over the time period 1990-2019.\n",
    "* These are not evenly spaced, but appear in clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "m = abs(returns).sort_values()[-100] # the top 100 returns are greater than this\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "mreturns = returns.loc[abs(returns) > m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "ret = pd.DataFrame(returns, index=returns.index)\n",
    "mret = pd.DataFrame(mreturns, index=mreturns.index)\n",
    "all = ret.join(mret, lsuffix='_caller', rsuffix='_other') # merge the data into one DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "all.plot(figsize=(15,6), style=['', 'ro'], legend=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Time series models\n",
    "* These phenomena typically become less pronounced as the time period between successive returns is increased. \n",
    "* For daily or weekly data, however, it is clear that a model needs to capture the time series variations, most importantly the time-varying volatility. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series Models: GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The GARCH model\n",
    "\n",
    "* The class of **GARCH (generalised autoregressive conditional heteroskedastic) models** incorporate time-varying volatility, autocorrelation in absolute / squared returns and fatter tails than suggested by the normal distribution (see Bollerlslev, 1986).\n",
    "* The GARCH(1,1) is the simplest and most widely used of the family of GARCH-type models.\n",
    "* A process $X=(X_t)_{t\\in \\mathbb{Z}}$ is a __GARCH(1,1) process__ if it is satisfies\n",
    "  $$\n",
    "  \\begin{array}{rcl}\n",
    "  X_t &=& \\sigma_t Z_t\\\\\n",
    "  \\sigma_t^2 &=& \\alpha_0 + \\alpha_1 X_{t-1}^2 + \\beta \\sigma_{t-1}^2,\n",
    "  \\end{array}\n",
    "  $$\n",
    "  where the **innovations** $Z_t$, $t=1, 2, \\ldots$ are iid standard normally distributed, and $\\alpha_0>0$, $\\alpha_1\\geq 0$ and $\\beta\\geq 0$. \n",
    "* In this model periods of high volatility tend to be __persistent__, that is, if either $|X_{t-1}|$ or $\\sigma_{t-1}$ are large, then $|X_t|$ has a tendency to be large as well, which in turn causes a high volatility. \n",
    "\n",
    "<font size=\"2\">\n",
    "    Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, pp. 31 (3), 307--327.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Properties of the GARCH model\n",
    "\n",
    "__Proposition__\n",
    "\n",
    "Let $X$ be a a GARCH(1,1) process satisfying $\\alpha_1+\\beta<1$. Then, for all $s,t\\in\\mathbb{Z}$,\n",
    "1. $\\mathbb{E}(X_t)=0$;\n",
    "2. $\\text{Var}(X_t)=\\displaystyle\\frac{\\alpha_0}{1-\\alpha_1-\\beta}$;\n",
    "3. the **autocorrelation** $\\mathbb E(X_t\\, X_s)/\\sqrt{\\text{Var}(X_t)\\, \\text{Var}(X_s)}$ is $0$ whenever $s\\not=t$;\n",
    "4. the variance of $X_t$ conditional on the information up to time $t-1$ is $\\sigma_t^2$;\n",
    "5. the kurtosis of $X_t$ is\n",
    "    $$    \\frac{\\mathbb E(X_t^4)}{\\mathbb E(X_t^2)^2} = \\frac{3\\, (1-(\\alpha_1+\\beta)^2)}{1-(\\alpha_1+\\beta)^2- 2\\, \\alpha_1^2}, $$\n",
    "    In particular, $X_t$ has a positive excess kurtosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variants of the GARCH process\n",
    "* The more general GARCH($p,q$) model is defined by setting the variance to\n",
    "$$\n",
    "\\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^p \\alpha_i X_{t-i}^2 + \\sum_{j=1}^q \\beta_j\n",
    "  \\sigma_{t-j}^2. \n",
    "$$\n",
    "* There are many extensions of GARCH processes (Integrated GARCH, GARCH with leverage, Threshold GARCH, ...). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting a GARCH model\n",
    "\n",
    "* Given a time series, such as the DAX returns, and postulating a GARCH model, we find the parameters that provide the \"best\" fit for the data. \n",
    "* The best fit is generally obtained via the method of __maximum likelihood__.\n",
    "* The `arch` library in Python will do this for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting a GARCH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "ret_demeaned=returns-returns.mean(); # de-mean process, i.e., adjust so that mean is zero\n",
    "am = arch_model(ret_demeaned, mean = 'Zero')\n",
    "res = am.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting a GARCH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting a GARCH model\n",
    "* The following parameters are obtained: $\\alpha_0=0.0283$, $\\alpha_1=0.0823$ and $\\beta=0.9019$. \n",
    "* All estimates are statistically significant ($p$-values<0.01). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pd.DataFrame(returns, index=returns.index)\n",
    "vol = pd.DataFrame(res.conditional_volatility, index=vol.index)\n",
    "all = ret.join(vol, lsuffix='_caller', rsuffix='_other') # merge the data into one DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting a GARCH model\n",
    "* The plot shows the DAX returns together with the fitted GARCH volatility. \n",
    "* The initial volatility is typically chosen as the time series' unconditional volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.plot(figsize=(15,6), style=['', 'r'],legend=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validating a GARCH model\n",
    "\n",
    "* To check the quality of the fit, one can compare the \"residuals\" $Z_t=X_t/\\sigma_t$ with a standard normal\n",
    "    distribution via a QQ-plot, see figure below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "residuals = (all[\"Close\"]/all[\"cond_vol\"]).dropna() # the residuals\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validating a GARCH model\n",
    "* This is what the residuals look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.plot(figsize=(15,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validating a GARCH model\n",
    "\n",
    "* In case the residuals do not fit the normal distribution, one may - in a second step - fit the residuals to a more appropriate distribution, such as the more heavy-tailed Student $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Volatility forecasting\n",
    "* One use of GARCH models is to forecast future volatility. \n",
    "* Given asset returns $x_0, \\ldots, x_t$ assume that a GARCH model has been fitted and that the condition     $\\alpha_1+\\beta<1$ (see Proposition above) is fulfilled. \n",
    "* A prediction of $\\sigma_{t+1}^2$ is given by\n",
    "$$\n",
    "\\hat\\sigma_{t+1}^2 = \\mathbb E(X_{t+1}^2|X_t, \\sigma_t) = \\alpha_0 +\n",
    "  \\alpha_1 X_t^2 + \\beta\\sigma_t^2,  \n",
    "$$\n",
    "and, more generally for one time period $h$ periods forward,\n",
    "$$\n",
    "\\hat\\sigma_{t+h}^2=\\mathbb E(X_{t+h}^2|X_t, \\sigma_t) = \\alpha_0\\,\n",
    "  \\sum_{i=0}^{h-1} (\\alpha_1+\\beta)^i + \n",
    "  (\\alpha_1+\\beta)^{h-1}\\, (\\alpha_1 X_t^2 + \\beta\\sigma_t^2). \n",
    "$$\n",
    "* A derivation of this formula is beyond the scope of the course. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Volatility forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmasq_f=[]\n",
    "tmp=[]\n",
    "alpha0=res.params[0]\n",
    "alpha1=res.params[1]\n",
    "beta=res.params[2]\n",
    "\n",
    "for i in range(0,251):\n",
    "    tmp.append((alpha1 + beta)**i)\n",
    "    \n",
    "for h in range(1,251):\n",
    "     sigmasq_f.append(alpha0 * np.sum(tmp[0:h]) + tmp[h-1] * (alpha1 * returns[-1]**2 \\\n",
    "                                                              + beta * res.conditional_volatility[-1]**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Volatility forecasting\n",
    "* The figure below shows the volatility forecast for the DAX return data.\n",
    "* The red line shows the unconditional standard deviation $\\displaystyle\n",
    "\\sqrt{\\frac{\\alpha_0}{1-\\alpha_1-\\beta}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconditional_vol=np.sqrt(res.params[0]/(1-res.params[1]-res.params[2]))*np.ones(len(sigmasq_f))\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.sqrt(sigmasq_f))\n",
    "plt.plot(unconditional_vol, 'r')\n",
    "plt.title('volatility forecast [%]')\n",
    "plt.xlabel('days');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_metadata": {
   "author": "Prof.\\ Dr.\\ Natalie Packham\\\\ Berlin School of Economics and Law\\\\ Computational Finance and FinTech\\\\ Summer Term 2020",
   "bib": "notebook.bib",
   "title": "Financial Time Series"
  },
  "livereveal": {
   "slideNumber": "c"
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
